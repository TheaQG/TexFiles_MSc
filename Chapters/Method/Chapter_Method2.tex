%!TeX root = Chapter_Method2
\documentclass[../../CompleteThesis2/Complete_2ndDraft]{subfiles}
\graphicspath{{../../Figures/}}
\begin{document}

\section[$\sigma$ Estimation Method][$\sigma$ Estimation Method]{$\sigma$ Estimation Method and Discussions}
\label{Sec:Method_SigmaMethod}

%Usually, when back-diffusing a depth series, some amplitude is regained on the signal, and some peaks and troughs, that might have been washed out from the diffusion might emerge when restoring the signal. These two properties thus makes it possible to connect the diffusion length to the years counted in that section (i.e. the number of peaks and troughs). Due to the knowledge of time passed from the Laki to the Tambora event, the data, along with the restoration techniques, permits a further investigation of the diffusion length in the given section. Thus, when back-diffusing, instead of just using the diffusion length estimate from the spectra, it is possible to examine which diffusion length that results in the correct number of years counted in that section. This property is what will be utilized in the following section, which focuses on finding the optimal diffusion length of a given section.

The general idea of the optimal $\sigma$ estimation method is to back diffuse a depth series defined on an interval where the time span (i.e. the number of peaks and troughs expected in that section) is known. This allows us to use the diffusion length as a tuning parameter, to find the diffusion length estimate which generates the right number of peaks and troughs and fulfills the imposed constraints in the back diffused depth series. If more than one diffusion length meet these constraints, the largest diffusion length to still fulfill the constraints is sought after. This diffusion length is then assumed to be the optimal guess on the diffusion length in that interval, which allows for a temperature estimate, following the temperature dependence of the diffusion length, as described in Section \ref{Sec:IceTheory}.

The algorithm consists of two modules, where one module describes the numerical back diffusion, given an inputted depth series, core specification and specific $\sigma_0$ estimate (which is either manually inputted or estimated from the spectral analysis). The flowchart describing the processes carried out in this module can be seen in Figure \ref{Fig:FlowchartBackDiffusion}. Many of the sections in this module are only necessary in the initialization of the algorithm as these parts do not change if the inputted diffusion length estimate is changed. In Figure \ref{Fig:FlowchartBackDiffusion} anything carried out above the \textit{Frequency Filters} block to the left is only computed once, and the same with anything to the right of it, except the $\sigma_0$ estimate. The density and diffusion profile calculations, the spectral analysis and the Wiener filter construction is inherent to the depth series alone, and these analyses are carried out as previously described in this thesis.

The second module is responsible for the optimization. This module examines the parameter space containing the diffusion length estimates, and utilizes a constrained direct search method to find the optimal diffusion length estimate. This method is illustrated through a flowchart in Figure \ref{Fig:METH_Flowchart_Optimization}. 

\subsection{Module 1: Initialization and Back Diffusion}
The first module of the algorithm, containing initialization and describing the general back diffusion method, is based on many of the aspects and models presented in the previous Chapters \ref{Chap:IceTheory} and \ref{Chap:SigAnalCompMeth}. Therefore the description of this module will focus on the work flow and not so much on the specific details of each process, as these have already been presented. 

The module takes an input of a measured depth series in a given interval, $d$, and the specifications concerning the drill site and the ice core in general. From here the work flow splits in two, one route(left) analyzing the depth series, and one(right) giving an estimate of $\sigma$ at that depth, based on models.

The right flow describes how the $\sigma_0$ estimate is given on the basis of the core specifications passed into the algorithm. First, a HL-density profile is modelled, based on the necessary input parameters of annual accumulation rate $A_0$ and drill site surface temperature $T_0$, and the optional inputs of surface density $\rho_0$ and measured depth versus density data. Secondly, this density profile is used to compute a diffusion length profile, by the use of the Iso-CFM. 

The modelled diffusion length profile is then used to find a theoretical $\sigma_0$ estimate. This then used as the standard deviation in the Gaussian transfer function filter, $\mathcal{G}$ used for deconvolution, unless a $\sigma_0$ is inputted manually. From the modelled diffusion length profile, it is also possible to choose not just a constant $\sigma_0$, but an option for a depth-varying diffusion length, $\sigma(z)$, used in the transfer function is also available. 

The left flow shows the analysis carried out on the measured depth series.


\begin{figure}[h]
	\begin{tikzpicture}[node distance=1.5cm, auto]
		\node(start) [startstop] {START};
		%----------------------------------------------------%
		\node(in1) [io, left of=start, xshift=-2.9cm, text width=4cm, align=center] {Measured depth series, $d$};
		\node(empty1) [below of=in1, yshift=0.12cm] {};
		\node(empty2) [below of=in1, xshift=-0.15cm] {};
		%		\node(in1pro1) [process, below of=in1, yshift=-0.5cm] {Spline interpolation};
		\node(in1pro2) [process, below of=in1, yshift=-2cm] {Spectral analysis of $\tilde{d}$};
		\node(decSpec1) [decision, below of=in1pro2, scale = 0.8, align=center] {DCT ?\\(Interpolation)};
		\node(decSpec2) [decision, left of=decSpec1, scale=0.8, xshift = -1.2cm, align=center] {FFT ?\\(Interpolation)};
		\node(decSpec3) [decision, right of=decSpec1, scale=0.8, xshift = 0.8cm] {NDCT ?};
		
		\node(in1pro3) [process, below of=decSpec1, text width=4cm, align=center] {Construct Wiener filter, $\tilde{F}$};
		
		\draw[arrow] (in1) -- (start);
		\draw[-] (start) |- (empty2);
		\draw[arrow] (empty1) -- (in1pro2);
		%		\draw[arrow] (in1pro1) -- (in1pro2);
		\draw[arrow] (in1pro2) -- (decSpec1);
		\draw[arrow] (in1pro2) -- (decSpec2);
		\draw[arrow] (in1pro2) -- (decSpec3);
		\draw[arrow] (decSpec1) -- (in1pro3);
		\draw[arrow] (decSpec2) -- (in1pro3);
		\draw[arrow] (decSpec3) -- (in1pro3);
		%	\draw[arrow] (in1pro3) -- (in1pro4);
		
		%----------------------------------------------------%
		
		\node(in2) [io, right of=start, xshift=2.5cm] {Core specs};
		\node(in2pro1) [process, below of=in2, yshift=-0.5cm] {Density profile};
		\node(in2pro2) [process, below of=in2pro1] {Diffusion profile};
		\node(in2pro3) [process, below of=in2pro2] {$\sigma_0$\textbf{ estimate}};
		\node(decSigma1) [decision, below of=in2pro3, scale = 0.8] {$\sigma_0 = \sigma_{\text{const}}$?};
		\node(decSigma2) [decision, right of=decSigma1, scale = 0.8, xshift=0.8cm] {$\sigma_0 = \sigma(z)$?};
		\node(decSigma3) [decision, left of=decSigma1, scale = 0.8, xshift=-0.8cm] {$\sigma_0 = \sigma_{\text{input}}$?};
		\node(in2pro4) [process, below of=decSigma1, text width=4cm, align=center] {Construct Transfer Function filter, $\tilde{\mathcal{G}}$};
		
		\draw[arrow] (in2) -- (start);
		\draw[arrow] (start) |- (in2pro1);
		\draw[arrow] (in2pro1) -- (in2pro2);
		\draw[arrow] (in2pro2) -- (in2pro3);
		\draw[arrow] (in2pro3) -- (decSigma1);
		\draw[arrow] (in2pro3) -- (decSigma2);
		\draw[arrow] (in2pro3) -- (decSigma3);
		\draw[arrow] (decSigma1) -- (in2pro4);
		\draw[arrow] (decSigma2) -- (in2pro4);
		\draw[arrow] (decSigma3) -- (in2pro4);
		%----------------------------------------------------%
		\node(pro0) [process, below of=start, yshift=-8.5cm, text width=4.5cm, align=center] {Frequency Filters, $\tilde{F}\cdot\tilde{\mathcal{G}}^{-1}$};
		\node(pro1) [process, below of=pro0, yshift=-0.4cm, ,align=center, text width=4cm, align=center] {\textbf{Deconvolution} $\mathcal{F}\left[\tilde{d}\cdot\tilde{F}\cdot\tilde{\mathcal{G}}^{-1}\right]$ \\(Uniform resampling)};
		\node(stop) [startstop, below of=pro1, yshift=-0.4cm] {\textbf{STOP}};
		\node(out1) [io, right of=stop, align=center, xshift=2.2cm, text width=2.5cm] {\footnotesize{Back diffused depth series, $D$}};
		\node(out2) [io, below of=out1, align=center] {\footnotesize{$\sigma_{\text{out}}$}};
		\draw[arrow] (in1pro3) |- (pro0);
		%		\draw[arrow] (decSigma1) |- (pro0);
		%		\draw[arrow] (decSigma2) |- (pro0);
		%		\draw[arrow] (decSigma3) |- (pro0);
		\draw[arrow] (in2pro4) |- (pro0);
		\draw[arrow] (pro0) -- (pro1);
		\draw[arrow] (pro1) -- (stop);
		\draw[arrow] (stop) -- (out1);
		\draw[arrow] (stop) |- (out2);
		
		%		\draw[arrow] (in2pro3) -| (pro0);
		
	\end{tikzpicture}
	\caption[Flowchart of initialization method.]{\small Flowchart of initialization method for back diffusion of a depth series given a diffusion length estimate.}
	\label{Fig:FlowchartBackDiffusion}
\end{figure}

\begin{figure}[h]
	\begin{tikzpicture}[node distance=1.5cm, auto]
		\node(pro1) [process, align=center] {$\sigma_0$ estimate};
		\node(in1) [io, right of=pro1, text width=2cm, align=center, xshift=3cm] {Inputs: $\Delta$, $\epsilon$, $N_{\sigma}$}; 
		\node(pro2) [process, text width = 4.3cm, align=center, below of=pro1, yshift=-1.8cm] {Construct $\bar{\sigma}_0$ grid, 
			\begin{align}
				\bar{\sigma} &= \begin{bmatrix}
					\sigma_0 - \left(\frac{N_{\sigma}}{2}-1\right)\Delta\\
					\vdots\\
					\sigma_0 - \Delta\\
					\sigma_0 \\
					\sigma_0 + \Delta\\
					\vdots \\
					\sigma_0 + \left(\frac{N_{\sigma}}{2}-1\right)\Delta
				\end{bmatrix}
				\nonumber
		\end{align}};%$\bar{\sigma}=$ [$\sigma_{-2}$, $\sigma_{-1}$, $\sigma_0$, $\sigma_{1}$, $\sigma_{2}$]};
		%\node(pro3) [process, below of=pro2, text width=4.5cm, align=center, yshift=-1.5cm] {Frequency Filters, $\tilde{F}\cdot\tilde{\mathcal{G}}(\bar{\sigma})$};
		
		
		\node(pro4) [process, minimum width=5cm, minimum height = 4cm, below of=pro2, yshift=-3.3cm, align=left] {};
		\node(for1) [below of=pro4, align=left, yshift=3.cm, xshift=-1.4cm] {\textbf{for} $\sigma$ \textbf{in} $\bar{\sigma}$:};
		\node(pro4dec1) [decision, below of=pro2, text width=5cm, scale=0.9, align=left, yshift=-3cm] {
			\hspace{6mm} Deconvolution, \\ 
			\hspace{6mm} $D=\mathcal{F}\left[\tilde{d}\cdot\tilde{F}\cdot\tilde{\mathcal{G}}(\sigma)^{-1}\right]$};
		\node(empty1) [left of=pro4dec1, xshift=-2.5cm, yshift=2.8cm] {Wiener filter, $\tilde{F}$};
		\node(pro4dec2) [decision, below of=pro4dec1, text width=4cm, scale=0.9, align=left] {Count $N_{peaks}$ in $D$,\\ under constraints};
		\node(pro4dec3) [draw, right of=pro4dec2, fill=Gray, xshift=4cm, text width=3.5cm, scale=0.8] {\textbf{if} $N_{peaks} \leq 33$: \\ 
			\hspace{5mm} $P_i=0$,\\ 
			\textbf{else}:\\ 
			\hspace{5mm} $P_i=1$};
		\node(pro4dec4) [decision, below of=pro4dec3, yshift=-0.3cm, scale=0.9] {$\bar{P}=[P_0,\, P_1,\,...,\, P_{N-1}]$};
		
		\node(in2) [io, left of=pro1, xshift=-4.5cm, text width= 2cm, align=center] {\textbf{constraints}};
		
		\node(empty2) [process, below of=pro4dec2, yshift=-3cm, minimum width=5cm, minimum height=3cm] {};
		\node(for2) [below of=pro4dec2, yshift=-2cm] {Construct new $\bar{\sigma}$ grid with:};
		\node(for2dec1) [decision, below of=for2, scale=0.9, text width = 4.5cm] {$\sigma_{min}=$ \textbf{max}$(\bar{\sigma}(P_i==0))$,\\ $\sigma_{max}=$ \textbf{min}$(\bar{\sigma}(P_i==1))$,\\
			$\Delta=\frac{\sigma_{max} - \sigma_{min}}{N_{\sigma}-1}$};
		\node(pro5) [process, left of=empty2, text width=4cm,xshift=-4cm, yshift=2cm] {\begin{align}
				\bar{\sigma} &= \begin{bmatrix}
					\sigma_{min}\\
					\sigma_{min} +\Delta\\
					\vdots\\
					\sigma_{max} - \Delta\\
					\sigma_{max}
				\end{bmatrix}
				\nonumber
		\end{align}};
		\node(if2) [below of=pro5, yshift=-1cm, scale=0.9] {\textbf{if} $\sigma_{max} - \sigma_{min} > \epsilon$};
		\node(stop) [startstop, right of=empty2, yshift=-1.5cm, xshift=3cm] {STOP};
		\node(if3) [above of=stop, yshift=0.5cm, scale=0.9] {\textbf{if} $\sigma_{max} - \sigma_{min} \leq \epsilon$};
		\node(out1) [io, below of=stop, xshift=0cm, text width= 3cm] {$\sigma_{\text{final}}=\sigma_{min}$, \\
			$D_{opt}=D({\sigma_{\text{final}}})$};
		
		
		\draw[arrow] (0,1) -- (pro1);
		\draw[arrow] (pro1) -- (pro2);
		\draw[arrow] (in1) |- (pro2);
		\draw[arrow] (pro2) -- (pro4);
		\draw[arrow] (-4,-5) |- (pro4dec1);
		\draw[arrow] (pro4dec1) -- (pro4dec2);
		\draw[arrow] (pro4dec2) -- (pro4dec3);
		\draw[arrow] (pro4dec3) -- (pro4dec4);
		\draw[arrow] (pro4dec4) -| (empty2);
		\draw[arrow] (empty2) -| (pro5);
		\draw[arrow] (empty2) -| (stop);
		\draw[arrow] (stop) -- (out1);
		\draw[arrow] (pro5) |- (pro4);
		\draw[arrow] (in2) |- (pro4dec2);
		%		\draw[arrow] (pro3) -- (pro4);
	\end{tikzpicture}
	\caption{For Method (optimization)}
	\label{Fig:METH_Flowchart_Optimization}
\end{figure}
%\begin{figure}
%	\begin{tikzpicture}[node distance=1.5cm, auto]
%		\node(start) [startstop] {START};
%		%----------------------------------------------------%
%		\node(in1) [io, left of=start, xshift=-2.5cm] {Depth series};
%		\node(empty1) [below of=in1, yshift=0.12cm] {};
%		\node(empty2) [below of=in1, xshift=-0.15cm] {};
%		%		\node(in1pro1) [process, below of=in1, yshift=-0.5cm] {Spline interpolation};
%		\node(in1pro2) [process, below of=in1, yshift=-2cm] {Spectral analysis};
%		\node(decSpec1) [decision, below of=in1pro2, scale = 0.8, align=center] {DCT ?\\(Interpolation)};
%		\node(decSpec2) [decision, left of=decSpec1, scale=0.8, xshift = -1.2cm, align=center] {FFT ?\\(Interpolation)};
%		\node(decSpec3) [decision, right of=decSpec1, scale=0.8, xshift = 0.8cm] {NDCT ?};
%		
%		\node(in1pro3) [process, below of=decSpec1] {Wiener filter};
%		
%		\draw[arrow] (in1) -- (start);
%		\draw[-] (start) |- (empty2);
%		\draw[arrow] (empty1) -- (in1pro2);
%		%		\draw[arrow] (in1pro1) -- (in1pro2);
%		\draw[arrow] (in1pro2) -- (decSpec1);
%		\draw[arrow] (in1pro2) -- (decSpec2);
%		\draw[arrow] (in1pro2) -- (decSpec3);
%		\draw[arrow] (decSpec1) -- (in1pro3);
%		\draw[arrow] (decSpec2) -- (in1pro3);
%		\draw[arrow] (decSpec3) -- (in1pro3);
%		%	\draw[arrow] (in1pro3) -- (in1pro4);
%		
%		%----------------------------------------------------%
%		
%		\node(in2) [io, right of=start, xshift=2.5cm] {Core specs};
%		\node(in2pro1) [process, below of=in2, yshift=-0.5cm] {Density profile};
%		\node(in2pro2) [process, below of=in2pro1] {Diffusion profile};
%		\node(in2pro3) [process, below of=in2pro2] {$\sigma_0$\textbf{ estimate}};
%		\node(decSigma1) [decision, below of=in2pro3, scale = 0.8] {$\sigma_0 = \sigma_{\text{const}}$?};
%		\node(decSigma2) [decision, right of=decSigma1, scale = 0.8, xshift=0.8cm] {$\sigma_0 = \sigma(z)$?};
%		\node(decSigma3) [decision, left of=decSigma1, scale = 0.8, xshift=-0.8cm] {$\sigma_0 = \sigma_{\text{input}}$?};
%		
%		
%		\draw[arrow] (in2) -- (start);
%		\draw[arrow] (start) |- (in2pro1);
%		\draw[arrow] (in2pro1) -- (in2pro2);
%		\draw[arrow] (in2pro2) -- (in2pro3);
%		\draw[arrow] (in2pro3) -- (decSigma1);
%		\draw[arrow] (in2pro3) -- (decSigma2);
%		\draw[arrow] (in2pro3) -- (decSigma3);
%		
%		%----------------------------------------------------%
%		\node(pro0) [process, below of=start, yshift=-6.5cm] {Frequency Filters};
%		\node(pro1) [process, below of=pro0,align=center] {\textbf{Deconvolution}\\(Uniform resampling)};
%		\node(stop) [startstop, below of=pro1] {\textbf{STOP}};
%		\node(out1) [io, right of=stop, align=center, xshift=2.2cm, text width=2.5cm] {\footnotesize{Back diffused depth series}};
%		\node(out2) [io, below of=out1, align=center] {\footnotesize{$\sigma_{\text{out}}$}};
%		\draw[arrow] (in1pro3) |- (pro0);
%		\draw[arrow] (decSigma1) |- (pro0);
%		\draw[arrow] (decSigma2) |- (pro0);
%		\draw[arrow] (decSigma3) |- (pro0);
%		\draw[arrow] (pro0) -- (pro1);
%		\draw[arrow] (pro1) -- (stop);
%		\draw[arrow] (stop) -- (out1);
%		\draw[arrow] (stop) |- (out2);
%		
%		%		\draw[arrow] (in2pro3) -| (pro0);
%		
%	\end{tikzpicture}
%	\caption[Flowchart of initialization method.]{\small Flowchart of initialization method for back diffusion of a depth series given a diffusion length estimate.}
%	\label{Fig:FlowchartBackDiffusion}
%\end{figure}

\subsection[Constrained Peak Detection]{Constrained Peak Detection}
\label{Subsec:CompMeths_PeakDetection_Constrained}
The main constraint when back diffusing these depth series is the number of years between the two volcanic events detected in the ice. The number of especially winters between the two events are fixed to $N_P=33$, as this number of winter is expected in the interval, even with a two month variation from the estimated event position. The counted number of summers can on the other hand vary a bit.\\
For better peak detection, a number of other constraints have been implemented. Since the data is a proxy for a continuous physical process, the temperature, it is reasonable to set up constraints representing some of the logical expectations for this type of signals. 

Firstly, restrictions may be demanded of the distance between peaks. The annual layer thickness, $\lambda_A$, may help with setting some limitations on the peak distances, as it is not likely for the average peak distance to be much smaller than $\lambda_A$. The peaks are expected to show the same annual cycle as the rest of the signal, representing summers. 

Secondly, for the prominence of the peaks, i.e. the amplitude of the signal at a depth, it is assumed that individual peaks may not have a prominence of less than a certain percentage of the standard deviation of the signal at the given depth. This makes certain that smaller peaks or troughs, maybe representing a warm period in a winter or a cold period in summer, are not counted as annual peaks or troughs. 
This is one way to constrain peak prominence, but a more efficient and accurate way might be to consider the amplitude of the entire ice core signal. As it may be assumed that the amplitude and prominence at a given depth, will be somewhat smaller than the prominence of the peaks at a shallower depth, due to the general diffusion in the firn. By analyzing the amplitude attenuation of the ice core, the average attenuation at a given depth could be used as a restriction for the peak prominence. This is something that would have been implemented, if time had allowed it.


Thirdly, a constraint on how the general pattern of the trough and peak detection must look is imposed. As the temperature variations represent the change from summer to winter, it is assumed that the general pattern must be to detect a peak $P$, then a trough $T$, then a peak $P$, and so on, creating a pattern of $...PTPTPTP...$. Since the deposition time of volcanic material in the ice is assumed to be Gaussian, the pattern is not restricted to start with either a peak or a trough, as this may vary when drawing a location from the distribution. Thus, the number of peaks is set at $N_P=33$ and the number of troughs is accepted with a variation, as long as the general pattern is intact.

Finally, the diffusion length estimate is kept at a positive value with an upper limit, that can be set manually, depending on the conditions of the site and the depth. 

The optimal parameter choices for the constraints are presented in:
\begin{table}[ht]
	\centering
	\begin{tabular}{lc}
		\toprule 	
		$N_P$ & 33\\[0.15cm]
		$N_T$ & $33\pm 1$\\[0.15cm]
		Peak(trough) prominence & 50 \% of $SD_{\text{signal}}$\\[0.15cm]
		Peak(trough) distance & 50 \% of $\lambda_A$\\[0.15cm]
		$[\sigma_{min}, \sigma_{max}]$ [cm] & [0, 15]\\[0.15cm]
		Pattern & $...PTPTPTPTP...$\\
		\bottomrule
	\end{tabular}
	\caption[Constraint parameters]{The general constraints used in the method to optimize the diffusion length estimate.}
	\label{Tab:ConstraintParams}
\end{table}




\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{SiteA_InterpBF_SpecificResamplings.png}
	\caption[Back diffused data, Site A, different resamplings before deconvolution.]{\small Site A, illustration of the effect of five different cubic spline resamplings before deconvolution. Original sample sizes lie in the interval [3.80; 4.00] [cm]. Resampling at smaller sample sizes show a tendency to restore some signal frequencies that are not necessarily inherit in the original signal. The resample should thus not be chosen too small as this would introduce some false signal into the results.}
	\label{Fig:COMPMETH_SiteA_interpBF_SpecificSamplings}
\end{figure}




\subsection[Corrections on $\sigma$][Corrections on $\sigma$]{Corrections on the Computed $\sigma$ estimate}
\label{Subsec:Method_SigmaMethod_SigmaCorrections}
As mentioned in Section \ref{Subsec:Ice_DiffusionAndDensification_Diffusion_TemperatureRecon}, the estimated diffusion length needs to be corrected for a number of different influences: the sampling diffusion, the ice diffusion at a given depth and the thinning function at a given drill site.

This section will describe how each of these influences have been taken care of in the work.

\subsubsection{Ice Diffusion, $\sigma_{\text{ice}}$}
\label{Subsubsec:Method_SigmaMethod_SigmaCorrections_IceDiffusion}

\subsubsection{Sampling Diffusion, $\sigma_{\text{dis}}$}
\label{Subsubsec:Method_SigmaMethod_SigmaCorrections_SamplingDiffusion}

\subsubsection{Thinning Function $S(z)$}
\label{Subsubsec:Method_SigmaMethod_SigmaCorrections_ThinningFct}



\subsection[Constraints][Constraints]{Constraints}
\label{Subsec:Method_SigmaMethod_Constraints}
This section will contain a walk through of the general differences between constrained and non-constrained optimization of the diffusion length. In Figure \ref{fig:SiteB_ConstVNoConst} an example of the results of the optimization with and without constraints can be seen.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{SiteB_ConstVNoConst.png}
	\caption[Site B Constrained vs. Unconstrained]{\small Example of how the imposed constraints effect the final diffusion length estimate for the Laki to Tamborad depth section of the core drilled at Site B. The black line shows the data, the green the back diffused data using a method with less constraints, and the blue shows the back diffused data when using the imposed constraint. The blue dots represents the peaks counted in the constrained method.}
	\label{fig:SiteB_ConstVNoConst}
\end{figure}


%\subsection[$1^{\text{st}}\sigma$ Estimate][$1^{\text{st}}\sigma$ Estimate]{$1^{\text{st}}$ Diffusion Length Estimate}
%\label{Subsec:Method_SigmaMethod_1stEstimate}

%\subsection[Optimal $\sigma$ Estimate][Optimal $\sigma$ Estimate]{Optimal Diffusion Length Estimate}
%\label{Subsec:Method_SigmaMethod_OptEstimate} 


\section[Testing and Stability][Testing and Stability]{Method Testing and Stability}
\label{Sec:Method_TestStab}

Throughout this section a number of different tests of the algorithm will be presented. The tests are performed to examine the stability of the method, the accuracy of the Laki and Tambora positions and how the choice of parameters(interpolation, spectral transform type, $\lambda_A$ estimation, peak prominence and peak distance) changes the resulting diffusion length estimate. 

\subsection[Constraints or No Constraints]{No constraints versus constraints}
\label{Subsec:Method_TestStab_ConstNoConst}


\subsection[Effects of Interpolations]{Effects of Interpolations}
\label{Subsec:METH_Interpolation}
\subsubsection[Interpolation 1]{Interpolation of Data Before Deconvolution}
\label{Subsubsec:METH_Interpolation_BFdecon}

The first interpolation is needed, if the fast spectral transforms FFT or FCT are used, as one of the conditions of the algorithms is that the data are evenly spaced. At first, this was implemented in the analysis, but this had the risk of excluding some information that might lie in the unevenly sampled data. Later, the method was abandoned in favor of implementing a nonuniform spectral transform (NUFT or NDCT), which is slower than the FFT and FCT, but carries all information from the unevenly sampled signal into the spectral domain. Luckily, this nonuniform transform needs only be carried out once, as the inverse transform, i.e. resampling in time domain, can be done uniformly without loss of information and any future spectral transforms can then be performed through FFT or FCT.

Even though the first interpolation method was later abandoned, some analysis was carried out with it to examine the effect of the size of the resampled, interpolated data on the final diffusion length estimate. Examples of a resampled signal can be seen in Figure \ref{Fig:COMPMETH_SiteA_DataSplineInterp} and Figure \ref{Fig:COMPMETH_SiteA_MultiSplineInterp}. Figure \ref{Fig:COMPMETH_SiteA_MultiSplineInterp} shows how sample resolution affects information from the signal. The higher sampling resolution, the more information is retained. But higher sampling resolution also means more data to be analyzed, which might slow down any analysis algorithms developed. This might create some headache if an entire ice core length of a couple thousand meters should be examined, but for this study only af few meters are of interest, and thus it should not create delays in the computation time.

To examine the effect of the resampling resolution on the final diffusion length estimate when conducting a spline interpolation before carrying out the back-diffusion, the full diffusion length analysis has been performed with 100 new interpolation resampling sizes in the range $[\Delta_{\text{min}};\Delta_{\text{max}}]$. This gives an idea of the stability of the method considering both sample size of the raw data and resampling by interpolation. The minimum and maximum interpolation samplings are presented in Table \ref{tab:InterpSamples} and an illustration of the test results can be seen in Figure \ref{Fig:COMPMETH_SamplingVsDiffLen_interpBF}. 

Figure \ref{Fig:COMPMETH_SamplingVsDiffLen_interpBF} shows that if the sampling size of the interpolation if decreased, it becomes difficult for the algorithm to determine a diffusion length that fulfills the constraints. This is due to the spectral transforms and back diffusion method being sensitive to smaller variations that the spline interpolation introduces to the signal, as can be seen in the first panel in Figure \ref{Fig:COMPMETH_SiteA_interpBF_SpecificSamplings}. Furthermore, Figure \ref{Fig:COMPMETH_SamplingVsDiffLen_interpBF} shows a less stable diffusion length estimate as the resampling size increases, and a general tendency to result in higher diffusion lengths as many features become washed out in the signal and needs more enhancement by cranking up the diffusion length estimate, see final panel in Figure \ref{Fig:COMPMETH_SiteA_interpBF_SpecificSamplings}. The interpolation before deconvolution is only necessary for running the method with the spectral transforms that are based on uniform sampling, i.e. the FFT and the DCT. For these two methods a choice of interpolation size can be made, but the general setting in the algorithm is to resample at the smallest sampling size found in the depth interval, $\Delta_{\text{min}}$.

\marginnote{%
	\footnotesize
	\centering
	\begin{tabular}{lccc}
		\toprule
		\textbf{Site} & $\Delta_{\text{min}}$& $\Delta_{\text{max}}$ & $\Delta_{\text{OG}}$\\
		& [cm] & [cm] & [cm] \\
		\midrule
		A & 1.0 & 10.6 & 3.8-4.0\\
		B & 1.0 & 11.7 & 3.8-4.0\\
		D & 1.0 & 12.0 & 3.7-3.9\\
		E & 1.0 & 11.4 & 4.1-4.4\\
		G & 1.0 & 10.3 & 4.0-4.2\\
		\bottomrule
	\end{tabular}
	\captionof{table}[Tested sample resolutions for resampling before back-diffusion.]{\footnotesize Minimal and maximal new sample resolution used for testing interpolation before back-diffusion. Each test is run with 100 different new sample resolutions between $\Delta_{\text{min}}$ and $\Delta_{\text{max}}$.}
	\label{tab:InterpSamples}
}[0.5cm]%
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{AllCores_DiffLenVdelta_InterpBF_const.png}
	\caption[Diffusion length versus resampling size before deconvolution, all cores.]{\small Diffusion length estimates versus resamples through cubic spline interpolation before deconvolution for Alphabet cores from sites A, B, D, E and G.}
	\label{Fig:COMPMETH_SamplingVsDiffLen_interpBF}
\end{figure}



\subsubsection[Interpolation 2]{Interpolation of Data After Deconvolution}
\label{Subsubsec:METH_Interpolation_AFdecon}
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{SiteA_InterpAF_SpecificResampling_BD.png}
	\caption[Back diffused data, Site A, different resamplings after deconvolution.]{\small Site A, effect of cubic spline interpolation after signal has been deconvoluted. The interpolation is introduced to make peak detection more stable.}
	\label{Fig:COMPMETH_SiteA_InterpAF_4samplings}
\end{figure}

The second interpolation is carried out after deconvoluting and back-diffusing the signal, but before detecting peaks. Splines are especially effective when trying to find features like peaks in data which underlying signal is continuous, smooth and differentiable, but the sampling is discrete and thus the data are discrete and non-smooth. The isotopic signal under examination here is assumed to be truly smooth and continuous throughout the core - unless any gaps are present. Thus the cubic spline interpolation is a good tool for estimating a higher resolution version of the final back-diffused data series to use for peak detection. This makes the detection of peaks and troughs more precise, as there might not be a discrete data point exactly at the top of a peak, but the spline interpolation then estimates where the most likely top of the peak must be, on the basis of the existing data. Examples of three different interpolation samplings are presented in Figure \ref{Fig:COMPMETH_SiteA_InterpAF_4samplings}. The effect of resampling after deconvolution on the final diffusion length estimate is illustrated in Figure \ref{Fig:COMPMETH_SamlingVsDiffLen}. 

As the actual isotopic signal is continuous and that the discretization is introduced by different measurement samplings, it is assumed that the spline interpolation after deconvolution results in a more likely peak detection, when decreasing the numerical resampling size. Therefore, for the method, a resample size of $\Delta_{\text{min}}/2$ is chosen for interpolation after back diffusion and before peak detection. In Figure \ref{Fig:COMPMETH_AllCores_SamplingVsDiffLen} The diffusion length estimate versus the resampling size after deconvolution and shows a convergence towards a fixed diffusion length as sampling size is decreased, and a much noisier diffusion length estimates as sampling size is increased. Furthermore, at certain larger sampling sizes the sought after pattern of peaks and troughs is not even reached. This resampling is carried out both for the FFT, DCT and NDCT spectral analysis methods, as the inverse NDCT can be resampled uniformly without any loss of information.



\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{AllCores_InterpAF_deltaVSdiffLen_BD.png}
	\caption[Diffusion length versus resampling size after deconvolution, all cores.]{\small Final diffusion length estimate, given new resample by cubic spline interpolation after deconvolution for Alphabet cores from sites A, B, D, E and G.. The original sample size interval is illustrated as black vertical lines.}
	\label{Fig:COMPMETH_AllCores_SamplingVsDiffLen}\textit{}
\end{figure}


\subsection[Spectral Transforms]{Spectral Transform's Effect on Diffusion Length}
\label{Subsec:Method_TestStab_SpecTrans}

\subsubsection{Visual Inspection}
\label{Subsubsec:Method_TestStab_SpecTrans_VisInspection}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{SiteA_SpecTrans_VisInspection.png}
	\caption[Qualitative Example of Spectral Transform's Effect on $\sigma$]{\small A visual example of the differences in final back diffused data when using different spectral transforms. }
	\label{fig:SiteA_SpecTrans_VisInspection}
\end{figure}


\subsubsection{Speed Examination}
\label{Subsubsec:Method_TestStab_SpecTrans_Speed}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{SiteA_SpecTrans_Speed.png}
	\caption[Speed and $\sigma$ Estimates, Spectral Transforms]{\small Diffusion length estimate along with speed of algorithm given the three different spectral transforms examined in this work. The volcanic event depths have been drawn from a Gaussian distribution with a width corresponding to ~1 month around the estimated mean depth of the event.}
	\label{fig:SiteA_SpecTrans_Speed}
\end{figure}




\subsection[LT locations]{Laki and Tambora as Gaussian Distributions}
\label{Subsec:Method_TestStab_LTlocations}



\subsubsection[Vary L and T]{Varying both Laki and Tambora Position}
\label{Subsubsec:Method_TestStab_LTlocations_LandT}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{SiteA_Vary_LandT.png}
	\caption[Diffusion Length Variations, Varying Laki and Tambora]{\small Diffusion length estimates when varying the depth locations of the volcanic events. The locations of both Laki and Tambora events have been drawn from Gaussian distributions as the ones presented in Section \ref{Sec:Data_VolcanicHorizons}.}
	\label{fig:SiteA_Vary_LandT}
\end{figure}



\subsubsection[Vary T]{Varying only Tambora Position}
\label{Subsubsec:Method_TestStab_LTlocations_T}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{SiteA_Vary_Lonly.png}
	\caption[Diffusion Length Variations, Varying only Laki]{\small Diffusion length estimates for Site A when varying only the Laki volcanic event. The locations are drawn from Gaussian distributions as the ones presented in Section \ref{Sec:Data_VolcanicHorizons}. The depth section is kept at a constant length, corresponding to the mean distance value, $\bar{d}_{\text{Laki}}-\bar{d}_{\text{Tambora}}$.}
	\label{fig:SiteA_Vary_Tonly}
\end{figure}



\subsubsection[Vary L]{Varying only Laki Position}
\label{Subsubsec:Method_TestStab_LTlocations_L}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{SiteA_Vary_Tonly.png}
	\caption[Diffusion Length Variations, Varying only Tambora]{\small Diffusion length estimates for Site A when varying only the Tambora volcanic event. The locations are drawn from Gaussian distributions as the ones presented in Section \ref{Sec:Data_VolcanicHorizons}. The depth section is kept at a constant length, corresponding to the mean distance value, $\bar{d}_{\text{Laki}}-\bar{d}_{\text{Tambora}}$.}
	\label{fig:SiteA_Vary_Lonly}
\end{figure}



\subsubsection[2 Month Variability]{Variation Corresponding to 2 Months}
\label{Subsubsec:Method_TestStab_LTlocations_2Month}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{SiteA_LandT_Gauss_2Mnth.png}
	\caption[Illustration of 2 Month Standard Deviation Variation of Volcanic Events Locations]{\small Illustration of the method implemented to manage the uncertainty of the exact depth location of the volcanic events. The method establishes a Gaussian distribution with a mean of the estimated middle of the volcanic event and a standard deviation of what corresponds to two months.}
	\label{fig:SiteA_LandT_Gauss_2Mnth}
\end{figure}

%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.95\textwidth]{SiteA_Vary_LandT_2mnth_DCT.png}
%	\caption[2 Month Variation of Event Locations, Site A]{\small 500 runs with locations of Laki and Tambora events drawn from a Gaussian distribution with a standard deviation of two months. Using DCT as spectral transform.}
%	\label{fig:SiteA_LandT_Gauss_2mnth_DCT}
%\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{SiteA_Vary_LandT_2mnth_NDCT.png}
	\caption[2 Month Variation of Event Locations, Site A]{\small 500 runs with locations of Laki and Tambora events drawn from a Gaussian distribution with a standard deviation of two months. Using NDCT as spectral transform.}
	\label{fig:SiteA_LandT_Gauss_2mnth_NDCT}
\end{figure}



\section[Upgrades][Upgrades]{Possible Algorithms Upgrades}
\label{Sec:Method_Upgrades}

This section will discuss ideas on how to improve on both the method and the algorithm

\subsection[Peak Detection]{Peak Detection}
\label{Subsec:Method_Upgrades_PeakDet}

\subsection[Optimization Routine]{Optimization Routine}
\label{Subsec:Method_Upgrades_OptiRout}









\end{document}
